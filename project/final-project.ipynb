{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This project is oriented toward building an effective system for generating color descriptions that are pragmatic in the sense that they would help a reader/listener figure out which color was being referred to in a shared context consisting of a target color (whose identity is known only to the describer/speaker) and a set of distractors.\n",
    "\n",
    "code courtsey for set up and initial analysis goes to notebook [hw_colors.ipynb](hw_colors.ipynb) and [colors_overview.ipynb](colors_overview.ipynb). Upto original system section has been resued from the homework assignment hw_colors.ipynb](hw_colors.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [All two-word examples as a dev corpus](#All-two-word-examples-as-a-dev-corpus)\n",
    "1. [Dev dataset](#Dev-dataset)\n",
    "1. [Random train–test split for development](#Random-train–test-split-for-development)\n",
    "1. [Improve the tokenizer](Improve-the-tokenizer)\n",
    "1. [Use the tokenizer](#Use-the-tokenizer)\n",
    "1. [Improve the color representations](mprove-the-color-representations)\n",
    "1. [Use the color representer](#Use-the-color-representer)\n",
    "1. [Initial model](#Initial-model)\n",
    "1. [GloVe embeddings ](GloVe-embeddings)\n",
    "1. [Try the GloVe representations](#Try-the-GloVe-representations)\n",
    "1. [Color context ](#Color-context)\n",
    "1. [First original system ](#First-original-system)\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [colors_overview.ipynb](colors_overview.ipynb) for set-up in instructions and other background details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_color_describer import ContextualColorDescriber\n",
    "from torch_color_describer import create_example_dataset\n",
    "\n",
    "import utils\n",
    "from utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dev corpus\n",
    "\n",
    "Working only with the two-word-only subset of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=2,\n",
    "    normalize_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples = list(dev_corpus.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subset has about one-third the examples of the full corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13890"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We __should__ worry that it's not a fully representative sample. Most of the descriptions in the full corpus are shorter, and a large proportion are longer. So this dataset is mainly for debugging, development, and general hill-climbing. All findings should be validated on the full dataset at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev dataset\n",
    "\n",
    "The first step is to extract the raw color and raw texts from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols, dev_texts = zip(*[[ex.colors, ex.contents] for ex in dev_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw color representations are suitable inputs to a model, but the texts are just strings, so they can't really be processed as-is. Question 1 asks you to do some tokenizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random train–test split for development\n",
    "\n",
    "For the sake of development runs, we create a random train–test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols_train, dev_rawcols_test, dev_texts_train, dev_texts_test = \\\n",
    "    train_test_split(dev_rawcols, dev_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the tokenizer\n",
    "\n",
    "The function `tokenize_example` simply splits its string on whitespace, remove punctuations and adds the required start and end symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(s):\n",
    "\n",
    "    # Improve me!\n",
    "    # Re-Used concepts from https://github.com/futurulus/colors-in-context/blob/master/tokenizers.py for implementation of  Monroe et al. 2017\n",
    "    import re\n",
    "    \n",
    "    WORD_RE_STR = r\"\"\"\n",
    "        (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "        |\n",
    "        (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "        |\n",
    "        (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "        |\n",
    "        (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "        |\n",
    "        (?:\\*{1,})                     # Asterisk runs.\n",
    "        |\n",
    "        (?:\\!{1,})                     # Exclamation runs.\n",
    "        |\n",
    "        (?:\\S)                         # Everything else that isn't whitespace. \n",
    "        \"\"\"\n",
    "\n",
    "    WORD_RE = re.compile(r\"(%s)\" % WORD_RE_STR, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "\n",
    "    def basic_unigram_tokenizer(s, lower=True):\n",
    "        words = WORD_RE.findall(s)\n",
    "        if lower:\n",
    "            words = [w.lower() for w in words]\n",
    "        return words\n",
    "    \n",
    "    ENDINGS = ['er', 'est', 'ish']\n",
    "\n",
    "    def heuristic_segmenter(word):\n",
    "        for ending in ENDINGS:\n",
    "            if word.endswith(ending):\n",
    "                return [word[:-len(ending)], ending]\n",
    "        return [word]\n",
    "\n",
    "    def heuristic_ending_tokenizer(s, lower=True):\n",
    "        words = basic_unigram_tokenizer(s, lower=lower)\n",
    "        return [seg for w in words for seg in heuristic_segmenter(w)]\n",
    "\n",
    "    \n",
    "    return [START_SYMBOL] + heuristic_ending_tokenizer(s) + [END_SYMBOL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'aqua', ',', 'teal', '</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_example(dev_texts_train[376])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'dark', 'er', 'blu', 'ish', '!', ',', 'dark', 'est', '.', '</s>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_example(\"Darker bluish!, darkest.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_seqs_train = [tokenize_example(s) for s in dev_texts_train]\n",
    "\n",
    "dev_seqs_test = [tokenize_example(s) for s in dev_texts_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only the train set to derive a vocabulary for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab = sorted({w for toks in dev_seqs_train for w in toks})\n",
    "\n",
    "dev_vocab += [UNK_SYMBOL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that the `UNK_SYMBOL` is included somewhere in this list. In test examples, words not seen in training will be mapped to `UNK_SYMBOL`. \n",
    "\n",
    "Conceptual note: If model's vocab is the same as the train vocab, then `UNK_SYMBOL` will never be encountered during training, so it will be a random vector at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the color representations\n",
    "\n",
    "The following functions do nothing at all to the raw input colors we get from the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_color_context(colors):\n",
    "    return [represent_color(color) for color in colors]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def represent_color(color):\n",
    "    \n",
    "    # translate HLS format into HSV format\n",
    "    H = color[0]\n",
    "    L = color[1]\n",
    "    S = color[2]\n",
    "    \n",
    "    h = H\n",
    "    v = L + 1.0 * S * min(L, 1-L)\n",
    "    s = 0\n",
    "    if v != 0:\n",
    "        s = 2.0 * (1 - 1.0 * L/v)\n",
    "        \n",
    "    # fourier transform\n",
    "    # h,s,v are already all normalized to be in [0,1] range \n",
    "    # due to colors.py\n",
    "    f_hat_real = []\n",
    "    f_hat_imag = []\n",
    "    for j, k, l in product((0, 1, 2), repeat=3):\n",
    "        f_jkl = np.exp(-2j * np.pi * (j*h + k*s + l*v))\n",
    "        f_hat_real.append(f_jkl.real)\n",
    "        f_hat_imag.append(f_jkl.imag)\n",
    "        \n",
    "    return f_hat_real + f_hat_imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your task__: Modify `represent_color_context` and/or `represent_color` to represent colors in a new way.\n",
    "    \n",
    "__Notes__:\n",
    "\n",
    "* You are not required to keep `represent_color`. This might be unnatural if you want to perform an operation on each color trio all at once.\n",
    "* For that matter, if you want to process all of the color contexts in the entire data set all at once, that is fine too, as long as you can also perform the operation at test time with an unknown number of examples being tested.\n",
    "\n",
    "* The Fourier-transform method of [Monroe et al. 2016](https://www.aclweb.org/anthology/D16-1243/) and [Monroe et al. 2017](https://transacl.org/ojs/index.php/tacl/article/view/1142) is a proven choice for our task. __It is not required that you implement this.__ However, if you decide to, you might find that the overly terse presentation in the paper is an obstacle. They key thing to see is that the notation $\\hat{f}_{jkl}$ is meant to specify a full coordinate system. Thus, you might do something like\n",
    "\n",
    "  ```\n",
    "from itertools import product\n",
    "for j, k, l in product((0, 1, 2), repeat=3):    \n",
    "    f_jkl = ...\n",
    "```\n",
    "\n",
    "  and collect these `f_jkl` values in a list of 27 values. Additionally, in Python, [`2j` produces a value with `real` and `imag` attributes](https://docs.python.org/3.7/library/cmath.html). Each element `f_jkl` should have these components. If you concatenate the `real` and `imag` parts of all the `f_jkl`, you will have a 54-dimensional representation, as in the paper. Remember to start with an HSV representation, and with $h$ in $[0, 360]$, $s$ in $[0, 200]$, and $v$ in $[0, 200]$ (or else do the scaling differently). Note that the values in our corpus are in HLS format, [which are easily converted to HSV](https://en.wikipedia.org/wiki/HSL_and_HSV#HSL_to_HSV).\n",
    "  \n",
    "* It's natural to ask why this Fourier transform is useful in the current context. This is a challenging question, and I don't have a complete answer, but here is an intuitive observation: if you consider the raw color representations to be embeddings, then you can see very quickly that our standard geometric notions are totally out of line with our intuitions about the colors themselves. For example, here is a plot where we simply vary the hue dimension while keeping the other dimensions constant:\n",
    "\n",
    "  <img src=\"fig/colors-hue-hls.png\" alt=\"A series of very different colors with cosine distances from orange ranging from 0 to 0.19\" />\n",
    "\n",
    "  I've printed the cosine distances from the leftmost color above each patch. They all look pretty similar. Now, you might say, well at least the distances are sort of proportional to how different the colors are from the first. However, that argument seems to crumble when we do the same experiment but now varying the saturation dimension:\n",
    "\n",
    "  <img src=\"fig/colors-saturation-hls.png\" alt=\"A series of very similar purple-ish colors with cosine distances from gray-purple ranging from 0 to 0.19\" />\n",
    "\n",
    "  These colors are all quite simular intuitively. Notice, though, that the cosine distances are identical to my first plot. Of course! Cosine distances doesn't care about the nature of these dimensions! The underlying color space is a cylinder, not a regular Euclidean 3d space!\n",
    "  \n",
    "  The Fourier transformation that we apply is remapping the colors into approximately the cylindrical space that we want. It is at least capturing some the circular/radial relationships that are inherent in the space. Thus, here are plots corresponding to the above, but now where the colors have been transformed for the cosine comparisons. \n",
    "  \n",
    "  First, the hue variation:\n",
    "  \n",
    "  <img src=\"fig/colors-hue-fourier.png\" alt=\"A series of very different colors with cosine distances from orange that are generally large (near 1.0)\" />\n",
    "\n",
    "  And then saturation:\n",
    "  \n",
    "  <img src=\"fig/colors-saturation-fourier.png\" alt=\"A series of very similar purple-ish colors with cosine distances from gray purple that seem aligned with visual color similarity\" />\n",
    "  \n",
    "  These distances seem much better aligned with intuitions to me, and I think that's quite general. Thus, even if our networks can in principle learn this remapping, it's very helpful to at least start them closer to where we want them to be.\n",
    "  \n",
    "  If you want to go one layer deeper, then the [Zhang and Lu 2002](https://www.sciencedirect.com/science/article/pii/S092359650200084X) paper that Monroe et al. 2016 cite is pretty intuitive. It's for the 2d case, but that actually makes the ideas somewhere more accessible, since they can easily plot the original and remapped feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the color representer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell just runs `represent_color_context` on the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cols_train = [represent_color_context(colors) for colors in dev_rawcols_train]\n",
    "\n",
    "dev_cols_test = [represent_color_context(colors) for colors in dev_rawcols_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our preprocessing steps are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  color_seqs = torch.FloatTensor(color_seqs)\n",
      "Stopping after epoch 121. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 33.336477279663086"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ContextualColorDescriber(\n",
       "\tbatch_size=1028,\n",
       "\tmax_iter=1000,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=True,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=50,\n",
       "\tembed_dim=50,\n",
       "\tembedding=None,\n",
       "\tfreeze_embedding=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_mod = ContextualColorDescriber(\n",
    "    dev_vocab,\n",
    "    early_stopping=True)\n",
    "dev_mod.fit(dev_cols_train, dev_seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_accuracy: 0.7912467607255975\n",
      "bleu: 0.6586667273916903\n"
     ]
    }
   ],
   "source": [
    "evaluation = dev_mod.evaluate(dev_cols_test, dev_seqs_test)\n",
    "print('listener_accuracy:', evaluation['listener_accuracy'])\n",
    "print('bleu:', evaluation['corpus_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embeddings\n",
    "\n",
    "The above model uses a random initial embedding, as configured by the decoder used by `ContextualColorDescriber`.  \n",
    "\n",
    "Below, we create a GloVe embedding based on model vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join('data', 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_embedding(vocab, glove_base_filename='glove.6B.50d.txt'): \n",
    "    glove_dict = utils.glove2dict(\n",
    "        os.path.join(GLOVE_HOME, glove_base_filename))\n",
    "    return utils.create_pretrained_embedding(glove_dict, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the GloVe representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if GloVe can help for our development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_glove_embedding, dev_glove_vocab = create_glove_embedding(dev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_mod_glove = ContextualColorDescriber(\n",
    "    dev_glove_vocab,\n",
    "    embedding=dev_glove_embedding,\n",
    "    early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 111. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 37.68464803695679"
     ]
    }
   ],
   "source": [
    "_ = dev_mod_glove.fit(dev_cols_train, dev_seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_accuracy: 0.7952778577598618\n",
      "bleu: 0.6600234778700497\n"
     ]
    }
   ],
   "source": [
    "evaluation = dev_mod_glove.evaluate(dev_cols_test, dev_seqs_test)\n",
    "print('listener_accuracy:', evaluation['listener_accuracy'])\n",
    "print('bleu:', evaluation['corpus_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw a small boost, possibly because tokeization scheme leads to good overlap with the GloVe vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color context\n",
    "\n",
    "In next model, we modify various components in `torch_color_describer.py`.\n",
    "\n",
    "Redesigned the model so that the target color (the final one in the context) is appended to each input token that gets processed by the decoder. We subclass the `Decoder` and `EncoderDecoder` from `torch_color_describer.py` so that we can build models that do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__: Modify the `Decoder` so that the input vector to the model at each timestep is not just a token representation `x` but the concatenation of `x` with the representation of the target color.\n",
    "\n",
    "__Notes__:\n",
    "\n",
    "* We might notice at this point that the original `Decoder.forward` method has an optional keyword argument `target_colors` that is passed to `Decoder.get_embeddings`. Because this is already in place, all you have to do is modify the `get_embeddings` method to use this argument.\n",
    "\n",
    "* The change affects the configuration of `self.rnn`, so you need to subclass the `__init__` method as well, so that its `input_size` argument accomodates the embedding as well as the color representations.\n",
    "\n",
    "* We can do the relevant operations efficiently in pure PyTorch using `repeat_interleave` and `cat`, but the important thing is to get a working implementation – you can always optimize the code later if the ideas prove useful to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ColorContextDecoder(Decoder):\n",
    "    def __init__(self, color_dim, *args, **kwargs):\n",
    "        self.color_dim = color_dim\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embed_dim + self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            batch_first=True)  \n",
    "\n",
    "\n",
    "    def get_embeddings(self, word_seqs, target_colors=None):\n",
    "        \"\"\"\n",
    "        You can assume that `target_colors` is a tensor of shape\n",
    "        (m, n), where m is the length of the batch (same as\n",
    "        `word_seqs.shape[0]`) and n is the dimensionality of the\n",
    "        color representations the model is using. The goal is\n",
    "        to attached each color vector i to each of the tokens in\n",
    "        the ith sequence of (the embedded version of) `word_seqs`.\n",
    "\n",
    "        \"\"\"\n",
    "        num_repeats = word_seqs.shape[1]\n",
    "        colors_repeated = torch.repeat_interleave(target_colors, num_repeats, dim=0)\n",
    "        colors_repeated_shaped = torch.reshape(colors_repeated, (target_colors.shape[0], num_repeats, target_colors.shape[1]))\n",
    "        return torch.cat((self.embedding(word_seqs), colors_repeated_shaped),2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import EncoderDecoder\n",
    "\n",
    "class ColorizedEncoderDecoder(EncoderDecoder):\n",
    "\n",
    "    def forward(self,\n",
    "            color_seqs,\n",
    "            word_seqs,\n",
    "            seq_lengths=None,\n",
    "            hidden=None,\n",
    "            targets=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.encoder(color_seqs)\n",
    "\n",
    "        num_contexts = color_seqs.shape[1]\n",
    "        target_colors = color_seqs[:,num_contexts-1,:]\n",
    "        output, hidden = self.decoder(word_seqs, seq_lengths=seq_lengths, hidden=hidden, target_colors=target_colors)\n",
    "\n",
    "\n",
    "        # Your decoder will return `output, hidden` pairs; the\n",
    "        # following will handle the two return situations that\n",
    "        # the code needs to consider -- training and prediction.\n",
    "        if self.training:\n",
    "            return output\n",
    "        else:\n",
    "            return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__: Finally, as in the examples in [Modifying the core model](colors_overview.ipynb#Modifying-the-core-model), you need to modify the `build_graph` method of `ContextualColorDescriber` so that it uses your new `ColorContextDecoder` and `ColorizedEncoderDecoder`. Here's starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import Encoder\n",
    "\n",
    "class ColorizedInputDescriber(ContextualColorDescriber):\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        encoder = Encoder(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim)\n",
    "\n",
    "        # Use your `ColorContextDecoder`, making sure\n",
    "        # to pass in all the keyword arguments coming\n",
    "        # from `ColorizedInputDescriber`:\n",
    "\n",
    "        decoder = ColorContextDecoder(\n",
    "            color_dim=self.color_dim,\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            embedding=self.embedding)\n",
    "\n",
    "\n",
    "        # Return a `ColorizedEncoderDecoder` that uses\n",
    "        # your encoder and decoder:\n",
    "\n",
    "        ##### YOUR CODE HERE\n",
    "        return ColorizedEncoderDecoder(encoder, decoder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Since these modifications are pretty intricate, you might want to use [a toy dataset](colors_overview.ipynb#Toy-problems-for-development-work) to debug it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that worked, then you can now try this model on SCC problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many options for your original system, which consists of the full pipeline – all preprocessing and modeling steps. You are free to use any model you like, as long as you subclass `ContextualColorDescriber` in a way that allows its `evaluate` method to behave in the expected way.\n",
    "\n",
    "So that we can evaluate models in a uniform way for the bake-off, we ask that you modify the function `evaluate_original_system` below so that it accepts a trained instance of your model and does any preprocessing steps required by your model.\n",
    "\n",
    "If we seek to reproduce your results, we will rerun this entire notebook. Thus, it is fine if your `evaluate_original_system` makes use of functions you wrote or modified above this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_original_system(trained_model, color_seqs_test, texts_test):\n",
    "    \"\"\"\n",
    "    Feel free to modify this code to accommodate the needs of\n",
    "    your system. Just keep in mind that it will get raw corpus\n",
    "    examples as inputs for the bake-off.\n",
    "\n",
    "    \"\"\"\n",
    "    # `word_seqs_test` is a list of strings, so tokenize each of\n",
    "    # its elements:\n",
    "    tok_seqs = [tokenize_example(s) for s in texts_test]\n",
    "\n",
    "    col_seqs = [represent_color_context(colors)\n",
    "                for colors in color_seqs_test]\n",
    "\n",
    "\n",
    "    # Optionally include other preprocessing steps here. Note:\n",
    "    # DO NOT RETRAIN YOUR MODEL AS PART OF THIS EVALUATION!\n",
    "    # It's a tempting step, but it's a mistake and will get\n",
    "    # you disqualified!\n",
    "\n",
    "    # The following core score calculations are required:\n",
    "    evaluation = trained_model.evaluate(col_seqs, tok_seqs)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `evaluate_original_system` works on test sets you create from the corpus distribution, then it will work for the bake-off, so consider checking that. For example, this would check that `dev_mod` above passes muster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_evaluation = evaluate_original_system(dev_mod, dev_rawcols_test, dev_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_accuracy: 0.7912467607255975\n",
      "bleu: 0.6586667273916903\n"
     ]
    }
   ],
   "source": [
    "print('listener_accuracy:', my_evaluation['listener_accuracy'])\n",
    "print('bleu:', my_evaluation['corpus_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. We also ask that you report the best **listener_accuracy** score your system got during development, just to help us understand how systems performed overall.\n",
    "\n",
    "<font color='red'>Please review the descriptions in the following comment and follow the instructions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model for GRU with multi layer and multi dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 2 of 1000; error is 55.665592193603516/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 3 of 1000; error is 55.7950382232666/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 4 of 1000; error is 54.303154945373535/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 5 of 1000; error is 54.832823276519775/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 6 of 1000; error is 53.77174234390259/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 7 of 1000; error is 53.688395500183105/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 8 of 1000; error is 54.09564256668091/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 9 of 1000; error is 53.927581787109375/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 10 of 1000; error is 53.07993841171265/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Finished epoch 11 of 1000; error is 53.346792221069336/home/vivek/AI-Graduate-program/CS224U/assigments/cs224u/torch_color_describer.py:680: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n",
      "Stopping after epoch 67. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 42.684147357940674"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7903829542182551\n",
      "0.6538640145474843\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This system has a few main components.\n",
    "# 1. Preprocessing: \n",
    "\n",
    "# 2. Encoder: \n",
    "# - GRU cell\n",
    "# - multi-layer [2, 3, 4, 5]\n",
    "# - hidden dim: [50, 100, 200, 300]\n",
    "\n",
    "# 3. Decoder: \n",
    "# - GRU cell\n",
    "# - multi-layer\n",
    "# - attention mechanisms\n",
    "# - hidden dim : [50, 100, 200, 300]\n",
    "# - word sequences are passed as GloVe embeddings and have color contexts appended\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_color_describer import Encoder, Decoder\n",
    "\n",
    "class DeepGRUEncoder(Encoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True)\n",
    "\n",
    "\n",
    "class DeepGRUDecoder(Decoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True)\n",
    "from torch_color_describer import EncoderDecoder\n",
    "\n",
    "class DeepGRUContextualColorDescriber(ContextualColorDescriber):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        self.num_layers = num_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build_graph(self):\n",
    "        encoder = DeepGRUEncoder(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)  \n",
    "\n",
    "        decoder = DeepGRUDecoder(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            embedding=self.embedding,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)  \n",
    "\n",
    "        return EncoderDecoder(encoder, decoder)\n",
    "\n",
    "mod_GRU_deep = DeepGRUContextualColorDescriber(\n",
    "    dev_glove_vocab,\n",
    "    embedding=dev_glove_embedding,\n",
    "    num_layers=4,\n",
    "    hidden_dim=300,\n",
    "    early_stopping=True)\n",
    "_ = mod_GRU_deep.fit(dev_cols_train, dev_seqs_train)\n",
    "\n",
    "evaluation = mod_GRU_deep.evaluate(dev_cols_test, dev_seqs_test)\n",
    "print('listener_accuracy:', evaluation['listener_accuracy'])\n",
    "print('bleu:', evaluation['corpus_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU based bidirectional Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 31. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 44.33427286148071"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_accuracy: 0.7474805643535848\n",
      "bleu: 0.3631851893157865\n"
     ]
    }
   ],
   "source": [
    "#Try bidirectional for both encoder and decoder\n",
    "\n",
    "# This system has a few main components.\n",
    "# 1. Preprocessing: \n",
    "\n",
    "# 2. Encoder: \n",
    "# - GRU cell\n",
    "# - multi-layer [2, 3, 4, 5]\n",
    "# - hidden dim: [50, 100, 200, 300]\n",
    "# - bidirectional\n",
    "\n",
    "# 3. Decoder: \n",
    "# - GRU cell\n",
    "# - multi-layer [2, 3, 4, 5, 8]\n",
    "# - attention mechanisms\n",
    "# - hidden dim : [50, 100, 200, 300]\n",
    "# - bidirectional\n",
    "# - word sequences are passed as GloVe embeddings and have color contexts appended\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_color_describer import Encoder, Decoder\n",
    "\n",
    "class DeepGRUEncoder(Encoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "\n",
    "class DeepGRUDecoder(Decoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)       \n",
    "        self.output_layer = nn.Linear(2*self.hidden_dim, self.vocab_size)\n",
    "\n",
    "        \n",
    "from torch_color_describer import EncoderDecoder\n",
    "\n",
    "class DeepGRUContextualColorDescriber(ContextualColorDescriber):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        self.num_layers = num_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build_graph(self):\n",
    "        encoder = DeepGRUEncoder(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)  \n",
    "\n",
    "        decoder = DeepGRUDecoder(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            embedding=self.embedding,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)  \n",
    "\n",
    "        return EncoderDecoder(encoder, decoder)\n",
    "\n",
    "mod_GRU_deep = DeepGRUContextualColorDescriber(\n",
    "    dev_glove_vocab,\n",
    "    embedding=dev_glove_embedding,\n",
    "    num_layers=3,\n",
    "    hidden_dim=300,\n",
    "    early_stopping=True)\n",
    "_ = mod_GRU_deep.fit(dev_cols_train, dev_seqs_train)\n",
    "\n",
    "evaluation = mod_GRU_deep.evaluate(dev_cols_test, dev_seqs_test)\n",
    "print('listener_accuracy:', evaluation['listener_accuracy'])\n",
    "print('bleu:', evaluation['corpus_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM based Encoder decoder baseline -  Bidirectional Encoder and multi layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 66. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 41.66188979148865"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_accuracy: 0.8139936654189461\n",
      "bleu: 0.6664774948006528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our system has a few main components.\n",
    "# 1. Preprocessing: \n",
    "\n",
    "# 2. Encoder: \n",
    "# - LSTM cell\n",
    "# - multi-layer [1, 2, 3]\n",
    "# - uni/bidirectional LSTM\n",
    "# - hidden dim [50, 100, 200, 300]\n",
    "\n",
    "# 3. Decoder: \n",
    "# - LSTM cell\n",
    "# - multi-layer [1, 4, 6]\n",
    "# - attention mechanisms\n",
    "# - hidden dim [50, 100, 200, 300]\n",
    "# - word sequences are passed as GloVe embeddings and have color contexts appended\n",
    "\n",
    "\n",
    "    class DeepLSTMEncoder(Encoder):\n",
    "        def __init__(self, *args, num_layers=2, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.num_layers = num_layers\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=self.color_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=True)\n",
    "\n",
    "\n",
    "        def forward(self, color_seqs):\n",
    "            \"\"\"\n",
    "            Parameters\n",
    "            ----------\n",
    "            color_seqs : torch.FloatTensor\n",
    "                The shape is `(m, n, p)` where `m` is the batch_size,\n",
    "                 `n` is the number of colors in each context, and `p` is\n",
    "                 the color dimensionality.\n",
    "            Returns\n",
    "            -------\n",
    "            hidden : torch.FloatTensor\n",
    "                These are the final hidden state of the RNN for this batch,\n",
    "                shape `(m, p) where `m` is the batch_size and `p` is\n",
    "                 the color dimensionality.\n",
    "            \"\"\"\n",
    "            output, hidden = self.rnn(color_seqs)\n",
    "\n",
    "            newhidden = torch.mean(hidden[0], dim=0, keepdim=True)\n",
    "            newcell = torch.mean(hidden[1], dim=0, keepdim=True)\n",
    "\n",
    "            return (newhidden, newcell)\n",
    "\n",
    "    class DeepLSTMDecoder(Decoder):\n",
    "        def __init__(self, color_dim, *args, num_layers=1, **kwargs):\n",
    "            self.color_dim = color_dim\n",
    "            self.num_layers = num_layers\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "            # Alter input_size of self.rnn\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=self.embed_dim + self.color_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers,\n",
    "                batch_first=True)#,\n",
    "                #bidirectional=True)  \n",
    "\n",
    "\n",
    "        def get_embeddings(self, word_seqs, target_colors=None):\n",
    "            \"\"\"\n",
    "            You can assume that `target_colors` is a tensor of shape\n",
    "            (m, n), where m is the length of the batch (same as\n",
    "            `word_seqs.shape[0]`) and n is the dimensionality of the\n",
    "            color representations the model is using. The goal is\n",
    "            to attached each color vector i to each of the tokens in\n",
    "            the ith sequence of (the embedded version of) `word_seqs`.\n",
    "            \"\"\"\n",
    "            num_repeats = word_seqs.shape[1]\n",
    "            colors_repeated = torch.repeat_interleave(target_colors, num_repeats, dim=0)\n",
    "            colors_repeated_shaped = torch.reshape(colors_repeated, (target_colors.shape[0], num_repeats, target_colors.shape[1]))\n",
    "            return torch.cat((self.embedding(word_seqs), colors_repeated_shaped),2)\n",
    "\n",
    "    # build an EncoderDecoder with custom encoder and decoders\n",
    "    class DeepLSTMContextualColorDescriber(ContextualColorDescriber):\n",
    "\n",
    "        def build_graph(self):\n",
    "\n",
    "            # use CustomEncoder here\n",
    "            encoder = DeepLSTMEncoder(\n",
    "                color_dim=self.color_dim,\n",
    "                num_layers = 3,\n",
    "                hidden_dim=300)\n",
    "\n",
    "            decoder = DeepLSTMDecoder(\n",
    "                color_dim=self.color_dim,\n",
    "                vocab_size=self.vocab_size,\n",
    "                embed_dim=self.embed_dim,\n",
    "                #num_layers=3,\n",
    "                hidden_dim=300)\n",
    "\n",
    "            # Return a `ColorizedEncoderDecoder` using Encoder and Decoder\n",
    "            return ColorizedEncoderDecoder(encoder, decoder)\n",
    "\n",
    "    mod_LSTM_deep = DeepLSTMContextualColorDescriber(\n",
    "        dev_glove_vocab,\n",
    "        embedding=dev_glove_embedding,\n",
    "        #dev_vocab,\n",
    "        #embed_dim=50,\n",
    "        early_stopping=True)\n",
    "\n",
    "    _ = mod_LSTM_deep.fit(dev_cols_train, dev_seqs_train)\n",
    "    evaluation = mod_LSTM_deep.evaluate(dev_cols_test, dev_seqs_test)\n",
    "    print('listener_accuracy:', evaluation['listener_accuracy'])\n",
    "    print('bleu:', evaluation['corpus_bleu'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM based Encoder decoder - Bidirectional and multi layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 50. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 43.846012592315674"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_accuracy: 0.6472790095018716\n",
      "bleu: 0.38923600929408014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This system has a few main components.\n",
    "# 1. Preprocessing: \n",
    "\n",
    "# 2. Encoder: \n",
    "# - LSTM cell\n",
    "# - multi-layer [2, 3, 4, 5, 6]\n",
    "# - bidirectional LSTM\n",
    "# - hidden dim: [50, 100, 200, 300]\n",
    "\n",
    "# 3. Decoder: \n",
    "# - LSTM cell\n",
    "# - multi-layer [2, 3, 4, 5, 6]\n",
    "# - bidirectional LSTM\n",
    "# - attention mechanisms\n",
    "# - hidden dim : [50, 100, 200, 300]\n",
    "# - word sequences are passed as GloVe embeddings and have color contexts appended\n",
    "class DeepLSTMEncoder(Encoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "\n",
    "    def forward(self, color_seqs):\n",
    "        output, hidden = self.rnn(color_seqs)\n",
    "        newhidden = hidden[0]\n",
    "        newcell = hidden[1]\n",
    "\n",
    "        return (newhidden, newcell)\n",
    "\n",
    "class DeepLSTMDecoder(Decoder):\n",
    "    def __init__(self, color_dim, *args, num_layers=1, **kwargs):\n",
    "        self.color_dim = color_dim\n",
    "        self.num_layers = num_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Alter input_size of self.rnn\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embed_dim + self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)  \n",
    "        #Redefine output_layer for bidirectional only. For unidirectional decoder, remove factor of 2\n",
    "        self.output_layer = nn.Linear(2*self.hidden_dim, self.vocab_size)\n",
    "\n",
    "\n",
    "    def get_embeddings(self, word_seqs, target_colors=None):\n",
    "        num_repeats = word_seqs.shape[1]\n",
    "        colors_repeated = torch.repeat_interleave(target_colors, num_repeats, dim=0)\n",
    "        colors_repeated_shaped = torch.reshape(colors_repeated, (target_colors.shape[0], num_repeats, target_colors.shape[1]))\n",
    "        return torch.cat((self.embedding(word_seqs), colors_repeated_shaped),2)\n",
    "    \n",
    "    def forward(self, word_seqs, seq_lengths=None, hidden=None, target_colors=None):\n",
    "        \n",
    "        embs = self.get_embeddings(word_seqs, target_colors=target_colors)\n",
    "        #print(\"decoder enter\", len(embs))\n",
    "        if self.training:\n",
    "            # Packed sequence for performance:\n",
    "            embs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                embs,\n",
    "                batch_first=True,\n",
    "                lengths=seq_lengths.cpu(),\n",
    "                enforce_sorted=False)\n",
    "            # RNN forward:\n",
    "            output, hidden = self.rnn(embs, hidden)\n",
    "            # Unpack:\n",
    "            output, seq_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "                output, batch_first=True)\n",
    "\n",
    "            # Output dense layer to get logits:\n",
    "            output = self.output_layer(output)\n",
    "            # Drop the final element:\n",
    "            output = output[:, : -1, :]\n",
    "            # Reshape for the sake of the loss function:\n",
    "            output = output.transpose(1, 2)\n",
    "            return output, hidden\n",
    "        else:\n",
    "            output, hidden = self.rnn(embs, hidden)\n",
    "            output = self.output_layer(output)\n",
    "            return output, hidden\n",
    "\n",
    "\n",
    "class ColorizedLSTMEncoderDecoder(EncoderDecoder):\n",
    "\n",
    "    def forward(self,\n",
    "            color_seqs,\n",
    "            word_seqs,\n",
    "            seq_lengths=None,\n",
    "            hidden=None,\n",
    "            targets=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.encoder(color_seqs)\n",
    "\n",
    "        num_contexts = color_seqs.shape[1]\n",
    "        target_colors = color_seqs[:,num_contexts-1,:]\n",
    "        output, hidden = self.decoder(word_seqs, seq_lengths=seq_lengths, hidden=hidden, target_colors=target_colors)\n",
    "\n",
    "\n",
    "        # Your decoder will return `output, hidden` pairs; the\n",
    "        # following will handle the two return situations that\n",
    "        # the code needs to consider -- training and prediction.\n",
    "        if self.training:\n",
    "            return output\n",
    "        else:\n",
    "            return output, hidden\n",
    "\n",
    "# build an EncoderDecoder with custom encoder and decoders\n",
    "class DeepLSTMContextualColorDescriber(ContextualColorDescriber):\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        # use CustomEncoder here\n",
    "        encoder = DeepLSTMEncoder(\n",
    "            color_dim=self.color_dim,\n",
    "            num_layers=6,\n",
    "            hidden_dim=300)\n",
    "\n",
    "        decoder = DeepLSTMDecoder(\n",
    "            color_dim=self.color_dim,\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_layers=6,\n",
    "            hidden_dim=300)\n",
    "\n",
    "        # Return a `ColorizedEncoderDecoder` using Encoder and Decoder\n",
    "        return ColorizedLSTMEncoderDecoder(encoder, decoder)\n",
    "\n",
    "mod_LSTM_deep_bidirectional = DeepLSTMContextualColorDescriber(\n",
    "    dev_glove_vocab,\n",
    "    embedding=dev_glove_embedding,\n",
    "    early_stopping=True)\n",
    "\n",
    "_ = mod_LSTM_deep_bidirectional.fit(dev_cols_train, dev_seqs_train)\n",
    "evaluation = mod_LSTM_deep_bidirectional.evaluate(dev_cols_test, dev_seqs_test)\n",
    "print('listener_accuracy:', evaluation['listener_accuracy'])\n",
    "print('bleu:', evaluation['corpus_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Based Encoder Decoder Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Try Encoder decoder Transformer\n",
    "import torch\n",
    "from transformers import EncoderDecoderModel, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-uncased\", \"bert-base-uncased\"\n",
    ")\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "input_ids = tokenizer(\"This is a really long text\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"This is the corresponding summary\", return_tensors=\"pt\").input_ids\n",
    "outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "loss, logits = outputs.loss, outputs.logits\n",
    "model.save_pretrained(\"bert2bert\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n",
    "generated = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: git-python==1.0.3 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: gitpython in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from git-python==1.0.3) (3.1.27)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from gitpython->git-python==1.0.3) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython->git-python==1.0.3) (5.0.0)\n",
      "Requirement already satisfied: rouge_score in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (0.0.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: nltk in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from rouge_score) (1.20.3)\n",
      "Requirement already satisfied: absl-py in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from rouge_score) (1.1.0)\n",
      "Requirement already satisfied: click in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from nltk->rouge_score) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: sacrebleu in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from sacrebleu) (1.20.3)\n",
      "Requirement already satisfied: portalocker in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from sacrebleu) (2.4.0)\n",
      "Requirement already satisfied: regex in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from sacrebleu) (2021.8.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from sacrebleu) (0.8.9)\n",
      "Requirement already satisfied: colorama in /home/vivek/anaconda3/envs/nlu/lib/python3.9/site-packages (from sacrebleu) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install git-python==1.0.3\n",
    "!pip install rouge_score\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "import os\n",
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")\n",
    "dev_corpus_t = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=2,\n",
    "    normalize_colors=False)\n",
    "dev_examples_t = list(dev_corpus_t.read())\n",
    "dev_rawcols_t, dev_texts_t = zip(*[[ex.colors, ex.contents] for ex in dev_examples_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for processing to bert as input\n",
    "\n",
    "def map_text_color_to_label():\n",
    "    text_t, color_seq_t, label_t, color_h_t, color_l_t, color_s_t = [], [], [], [], [], []\n",
    "    for index in range(len(dev_texts_t)):\n",
    "        for seq in range(3):\n",
    "            text_t.append(dev_texts_t[index])\n",
    "            color_seq_t.append(dev_rawcols_t[index][seq])\n",
    "            color_h_t.append(int(dev_rawcols_t[index][seq][0]))\n",
    "            color_l_t.append(int(dev_rawcols_t[index][seq][1]))\n",
    "            color_s_t.append(int(dev_rawcols_t[index][seq][2]))\n",
    "            if seq == 2:\n",
    "                label_t.append(1)\n",
    "            else:\n",
    "                label_t.append(0)\n",
    "\n",
    "    return text_t, color_seq_t, label_t, color_h_t, color_l_t, color_s_t\n",
    "\n",
    "data_text_t, data_color_t, data_label_t, color_h_t, color_l_t, color_s_t = map_text_color_to_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   text            color_seq  color_h  color_l  color_s  label\n",
      "0           Medium pink  [302.0, 50.0, 86.0]      302       50       86      0\n",
      "1           Medium pink  [291.0, 50.0, 59.0]      291       50       59      0\n",
      "2           Medium pink  [301.0, 50.0, 57.0]      301       50       57      1\n",
      "3           Mint green.   [57.0, 50.0, 64.0]       57       50       64      0\n",
      "4           Mint green.  [239.0, 50.0, 87.0]      239       50       87      0\n",
      "...                 ...                  ...      ...      ...      ...    ...\n",
      "41665      bright green   [284.0, 50.0, 7.0]      284       50        7      0\n",
      "41666      bright green  [126.0, 50.0, 47.0]      126       50       47      1\n",
      "41667  green..no yellow   [74.0, 50.0, 90.0]       74       50       90      0\n",
      "41668  green..no yellow   [159.0, 50.0, 1.0]      159       50        1      0\n",
      "41669  green..no yellow   [80.0, 50.0, 78.0]       80       50       78      1\n",
      "\n",
      "[41670 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(data_text_t, data_color_t, color_h_t, color_l_t, color_s_t, data_label_t)), columns=['text', 'color_seq', 'color_h', 'color_l', 'color_s', 'label'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "filepath = Path('./out.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/vivek/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/vivek/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/vivek/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vivek/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_text_t = []\n",
    "for index in range(len(data_text_t)):\n",
    "    tokenized_text_t.append(tokenizer(data_text_t[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (df['text'].copy())#.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df2[:train_split]\n",
    "test_dataset = df2[train_split:]\n",
    "filepath = Path('./out_train.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "train_dataset.to_csv(filepath)\n",
    "filepath = Path('./out_test.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "test_dataset.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-11beba966d7d7eb2\n",
      "Reusing dataset csv (/home/vivek/.cache/huggingface/datasets/csv/default-11beba966d7d7eb2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620891640ec3455691872135bc093a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e3d04b6a4c8a71b6\n",
      "Reusing dataset csv (/home/vivek/.cache/huggingface/datasets/csv/default-e3d04b6a4c8a71b6/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d1a7402bb640719ef4dc9baaa9ec09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scc_data = datasets.load_dataset('csv', data_files='out_train.csv')\n",
    "test_scc_data = datasets.load_dataset('csv', data_files='out_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/vivek/.cache/huggingface/datasets/csv/default-11beba966d7d7eb2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-d324060cde868422.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878f7ee1065743619797f3af0755162e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4167 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch\n",
    "#train_data = train_scc_data[:32]\n",
    "# batch_size = 16\n",
    "batch_size=1\n",
    "\n",
    "train_data = train_scc_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    #remove_columns=[\"color_seq\", \"color_h\", \"color_l\", \"color_s\", \"label\"]\n",
    "    remove_columns=[\"index\"],\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "val_data = test_scc_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"index\"],\n",
    "    #remove_columns=[\"color_seq\", \"color_h\", \"color_l\", \"color_s\", \"label\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=5,\n",
    "    save_steps=10000,\n",
    "    eval_steps=10000,\n",
    "    num_train_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: text. If text are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 37503\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 75006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3311' max='75006' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3311/75006 3:00:57 < 65:20:35, 0.30 it/s, Epoch 0.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3295888/3758901759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data['train'],\n",
    "    eval_dataset=val_data['train'],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
